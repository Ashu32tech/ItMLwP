{"cells":[{"cell_type":"code","source":["%sql show tables"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%sql drop table tmp"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sql create table tmp as select c1||':'||c2||':'||c3||':'||c4||':'||c5||':'||c6||':'||c7 as col FROM weblog"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sql select * from tmp"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# create pyspark dataframe from tmp table\ndf = spark.sql(\"SELECT col FROM tmp\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# display pyspark dataframe\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# use regex to separate the fields\nfrom pyspark.sql.functions import *\ndf = df.select(regexp_replace(col('col'), ' \"', ',\"'))\ndf = df.select(col(\"regexp_replace(col,  \\\", ,\\\")\").alias(\"col\"))\ndf = df.select(regexp_replace(col('col'), '\" ', '\",'))\ndf = df.select(col(\"regexp_replace(col, \\\" , \\\",)\").alias(\"col\"))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# split the flields 1\nsplit_col = split(df['col'], ',\"')\ndf = df.withColumn('c1', split_col.getItem(0))\ndf = df.withColumn('request', split_col.getItem(1))\ndf = df.withColumn('c3', split_col.getItem(2))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# split the flields 2\nsplit_col = split(df['c3'], '\",')\ndf = df.withColumn('user_agent', split_col.getItem(0))\ndf = df.withColumn('c5', split_col.getItem(1))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# split the flields 3\nsplit_col = split(df['c5'], ' ')\ndf = df.withColumn('ssl_cipher', split_col.getItem(0))\ndf = df.withColumn('ssl_protocol', split_col.getItem(1))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# split the flields 4\nsplit_col = split(df['c1'], ' ')\ndf = df.withColumn('timestamp', split_col.getItem(0))\ndf = df.withColumn('elb', split_col.getItem(1))\ndf = df.withColumn('client_port', split_col.getItem(2))\ndf = df.withColumn('backend_port', split_col.getItem(3))\ndf = df.withColumn('request_processing_time', split_col.getItem(4))\ndf = df.withColumn('backend_processing_time', split_col.getItem(5))\ndf = df.withColumn('response_processing_time', split_col.getItem(6))\ndf = df.withColumn('elb_status_code', split_col.getItem(7))\ndf = df.withColumn('backend_status_code', split_col.getItem(8))\ndf = df.withColumn('received_bytes', split_col.getItem(9))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# drop the unprocessed fields\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\n\ndf = reduce(DataFrame.drop, ['c1','c3','c5'], df)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# display the dataframe\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"Rough","notebookId":335305396330701},"nbformat":4,"nbformat_minor":0}
